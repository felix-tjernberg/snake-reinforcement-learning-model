{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines 3\n",
    "\n",
    "## Swig installation\n",
    "\n",
    "Swig is \"only\" needed to run stable baselines 3 base environments and is not needed for custom game environments\n",
    "- [Install swig before installing baselines 3 on windows](https://gist.github.com/felix-tjernberg/8bc7313ad1a0de136789f11d7ae7acd3) \n",
    "- Install swig on mac before installing baselines 3 -> `brew install swig`\n",
    "\n",
    "## Tutorial references\n",
    "- [mustafa Video tutorial](https://www.youtube.com/watch?v=5dxJEXCjruE)\n",
    "- [sentdex Video tutorial](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)\n",
    "- [sentdex Written tutorial](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the goal was to learn the basic environment setup, so following cells are just code from the [first part of the sentex video tutorial](https://www.youtube.com/watch?v=XbWhJdQgi7E&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\") # Using CarPole-v1 as I could not get LunarLander to work on windows :/\n",
    "environment.reset()\n",
    "print(\n",
    "    f\"\"\"\n",
    "    sample action: {environment.action_space.sample()}\n",
    "    observation space: {environment.observation_space.shape}\n",
    "    sample observation: {environment.observation_space.sample()}\n",
    "    \"\"\"\n",
    ")\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "environment.reset()\n",
    "for step in range(100):\n",
    "    environment.render()\n",
    "    environment.step(environment.action_space.sample())\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset()\n",
    "for step in range(10):\n",
    "\tenvironment.render()\n",
    "\tobs, reward, done, info = environment.step(environment.action_space.sample())\n",
    "\tprint(obs, reward, done, info)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "A2C_model = A2C('MlpPolicy', environment, verbose=1)\n",
    "A2C_model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "\tobs = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\t# pass observation to model to get predicted action\n",
    "\t\taction, _states = A2C_model.predict(obs)\n",
    "\n",
    "\t\t# pass action to environment and get info back\n",
    "\t\tobs, rewards, done, info = environment.step(action)\n",
    "\n",
    "\t\t# show the environment on the screen\n",
    "\t\tenvironment.render()\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[In part 2 sentdex taught how to save and load models and run a local tensorboard website](https://www.youtube.com/watch?v=dLP-2Y6yu70&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "model_directories = ['models/A2C', 'models/PPO']\n",
    "log_directory = 'logs'\n",
    "\n",
    "for directory in model_directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "\n",
    "A2C_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory) # Switching to verbose 0 so we can train more without flooding the cell output\n",
    "PPO_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 10000\n",
    "def train_and_save_model(model, model_directory):\n",
    "    model_name = model_directory.split('/')[1]\n",
    "    for index in range(1,11):\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, reset_num_timesteps=False, tb_log_name=model_name)\n",
    "        model.save(f'{model_directory}/{TOTAL_TIMESTEPS*index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_model(A2C_model, model_directories[0])\n",
    "train_and_save_model(PPO_model, model_directories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching local tensorboard website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pipenv shell` then `tensorboard --logdir=logs` in terminal to serve a tensorboard website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and running a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model_path = f'{model_directories[0]}/90000.zip'\n",
    "\n",
    "loaded_PPO_model = PPO.load(ppo_model_path, environment)\n",
    "\n",
    "for episode in range(10):\n",
    "\tobservation = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\tenvironment.render()\n",
    "\t\taction, _ = loaded_PPO_model.predict(observation)\n",
    "\t\tobservation, reward, done, info = environment.step(action)\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making my snake game agent compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I [continued to watch part 3 from sentdex](https://www.youtube.com/watch?v=uKnjGn8fF70&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1) and I also found [this video from mustafa](https://www.youtube.com/watch?v=5dxJEXCjruE) to see what was required when setting up the training environment\n",
    "\n",
    "What I noted from those videos was that they coded the snake game into the training environment class, which in my opinion makes the environment code very hard to read and edit\n",
    "\n",
    "My goal then became to rewrite the snake game into a class that is agent compatible, ie it can be ran from a training environment\n",
    "\n",
    "First I tried to add a variable that says if the game is agent driven and keep the human playability of the game which you can see in [this commit](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/commit/16e9a5915c538b5bcdf9743f758c351f47f97f6f). My thought process was that it might be cool if you could start the game either in agent mode or in human mode\n",
    "\n",
    "In the [next commit](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/commit/4526e9593bccd105e882c5b1e743be105b5880a0) I realized that it was just better to have a agent playable version and a human playable version of the game, to show that an agent could play I rewrote `if __name__ == \"__main__\":` so that it initializes the game and then tests if agent input works\n",
    "\n",
    "This accomplishes my goal of making both the snake game and training environment more readable and editable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal was to get the environment up and running in the most basic way like mustafa did and sentdex did in their videos, I combined mustafa's first observation with sentdex's observation code: `[head.x, head.y, food_delta_y, food_delta_x, body_x, body_y]`\n",
    "\n",
    "I also added body_x and body_y as observations. Which is the sum of all coordinates x and y which in my mind will create a vector for body size, shape and position: The thought is that hopefully model only need information about the body and not history of previous moves\n",
    "\n",
    "I also choose to do food_delta from sentdex's video instead of exact position from mustafa to help the model figure out if the food is up, down, left or right in relation to the head, which hopefully reduces the need for the model to figure that part out\n",
    "\n",
    "The check environment scripts that sentdex showed where very useful, especially the check_environment_visually (As I renamed his doublecheckenv script to). When I ran this script I saw that none of the inputs worked as I intended\n",
    "\n",
    "I solved the input issue by changing the Direction class into a dictionary as a class attribute instead in the snake game, so I learned the lesson of always checking the gym environment visually is very important which sentdex also said in his video :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and reward code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_game_state(self):\n",
    "    snake_head = self.snake_game.head\n",
    "    food = self.snake_game.food\n",
    "    food_delta_x = snake_head.x - food.x\n",
    "    food_delta_y = snake_head.y - food.y\n",
    "\n",
    "    all_body_x = [coordinate.x for coordinate in self.snake_game.body]\n",
    "    all_body_y = [coordinate.y for coordinate in self.snake_game.body]\n",
    "\n",
    "    return array(\n",
    "        [\n",
    "            snake_head.x,\n",
    "            snake_head.y,\n",
    "            food_delta_x,\n",
    "            food_delta_y,\n",
    "            sum(all_body_x),\n",
    "            sum(all_body_y),\n",
    "        ],\n",
    "        dtype=float32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward the agent for eating food and punish for game over\n",
    "if self.snake_game.game_over:\n",
    "    self.reward = self.snake_game.score - 10\n",
    "else:\n",
    "    self.reward = self.snake_game.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the first version was not very good it looked decent on [the graph](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/blob/main/models/saved_models/PPO_v1_1653236185_960000.png), but when I ran the model it was just following some [spinning patterns](https://youtu.be/PGNiXGX2nLU?t=60) like mustafa warned it would as it really does not want to die\n",
    "\n",
    "But the goal was to get training and loading of models to work\n",
    "\n",
    "I'm saving the best model and logs for each environment change for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding score to the logger object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm going to modify how reward is awarded when trying to solve snake it is not very useful to only compare ep_mean_reward as rewards will be awarded differently between models, for example some models might get more reward than 1 when eating food. Something that is better to compare is actual game score as that is really what matters anyways\n",
    "\n",
    "This required me to create a [custom callback](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#custom-callback) that adds game_score to the logger object with the .record method\n",
    "\n",
    "To get the score I started to add snake_game.score to the infos object, this can then be recorded to the logger using self.locals in _on_step method. (I have no idea why infos object is a list)\n",
    "\n",
    "More interesting to things to note:\n",
    "1. ep_rew_mean is calculated over 100 episodes for PPO at least\n",
    "2. I also might want to investigate [adding a is_success](https://stable-baselines3.readthedocs.io/en/master/common/logger.html#eval) to the infos object to see if the agent reached max score, might add this when I get closer to actually beating the game regularly \n",
    "3. There is a lot more you can do with [custom callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#custom-callback) (you have access to quite a lot of info in the self parameter, as they describe in the comments of the __init__ function), [like this person did and saves the model when agent reaches a new high score](https://github.com/DLR-RM/stable-baselines3/issues/506#issue-938981510) using the global property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add pygame.event.pump()\n",
    "\n",
    "Something I did not notice before was that the game window would appear to freeze when running and training the model\n",
    "\n",
    "I found out the issue is in the agent version of the game because it does not run pygame.event.get() as no key presses are expected\n",
    "\n",
    "This means that pygame is not making any calls to the event que. Another way to make calls to the event que is to run [pygame.event.pump()](https://www.pygame.org/docs/ref/event.html#pygame.event.pump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have set up the environment it is time to try different observations and rewards, I'm first going to try Mustafas manhattan distance approach and see what kind of results I can get\n",
    "\n",
    "I will modify Mustafa's approach by only rewarding the agent once for each manhattan distance closer to the food and not punish for getting further away like Mustafa did \n",
    "\n",
    "My thinking is that punishing the agent for getting further away from the food might discourage the agent to take longer paths when the body gets longer which makes it more likely to trap itself\n",
    "\n",
    "One possible issue with this method is that the agent will figure out that it can go as far away from the food as possible to be able to get all manhattan distance rewards, to prevent this I will only reward the agent if it gets closer to than the first manhattan distance after food spawn, I'm also going to track the manhattan distances so the agent only can be rewarded once per distance\n",
    "\n",
    "I will also remove the minus score from game over and only give minus reward if the agent gets game over from eating itself (Mustafas assignment)\n",
    "\n",
    "Also throwing away the sum(all_body_x/y) observations, I realized that this does not give the intended body information. For example if the snake body takes x positions of \\[1,2,3] it is the same as if the snake body takes the x position of \\[6] (x positions \\[1,2,3] == x position \\[6])\n",
    "\n",
    "I'm also adding a max amount of allowed steps to stop the [spinning patterns](https://youtu.be/PGNiXGX2nLU?t=60), this will be 42x42 because you only need to visit each coordinate once if you take the optimal path between each food when snake body starts to fill up the whole screen. I'm also giving the amount of steps taken to the agent as observation so it can take that into account\n",
    "\n",
    "A friend also alerted me that the game will crash if you reach max score, this is because of when the you reached max body size the _place_food method will end in an infinite loop when trying to spawn a new food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\n",
      "Manhattan distance test:\n",
      "Coordinate(x=210, y=220) Coordinate(x=110, y=240)\n",
      "120\n",
      "Coordinate(x=200, y=220) Coordinate(x=110, y=240)\n",
      "110\n",
      "Coordinate(x=200, y=210) Coordinate(x=110, y=240)\n",
      "120\n",
      "Coordinate(x=210, y=210) Coordinate(x=110, y=240)\n",
      "130\n",
      "1761\n"
     ]
    }
   ],
   "source": [
    "from snake_game.snake_game_agent_version import Coordinate, SnekGame\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def manhattan_distance(coordinate_one: Coordinate, coordinate_two: Coordinate):\n",
    "    \"\"\"Takes two coordinates and returns the manhattan distance between them\"\"\"\n",
    "    return int(abs(coordinate_one.x - coordinate_two.x) + abs(coordinate_one.y - coordinate_two.y))\n",
    "\n",
    "\n",
    "snake_game_instance = SnekGame()\n",
    "snake_game_instance.display_ui = True\n",
    "\n",
    "\n",
    "def test_distance(distance_function):\n",
    "    def make_move(direction):\n",
    "        snake_game_instance.agent_action = snake_game_instance.DIRECTION[direction]\n",
    "        snake_game_instance.game_tick()\n",
    "        head, food = snake_game_instance.head, snake_game_instance.food\n",
    "        print(head, food)\n",
    "        print(distance_function(head, food))\n",
    "        sleep(1)\n",
    "\n",
    "    for move in [\"down\", \"left\", \"up\", \"right\"]:\n",
    "        make_move(move)\n",
    "\n",
    "\n",
    "print(\"\\nManhattan distance test:\")\n",
    "test_distance(manhattan_distance)\n",
    "print(snake_game_instance.max_score)\n",
    "snake_game_instance.quit_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Version 2 taught the agent to go for the food quite well, the agent has also learned some strategies to avoid it's body but they work very poorly\n",
    "\n",
    "Some strategies I noted are making O's and trying to make lines close to foods\n",
    "\n",
    "This is quite impressive as the only information the agent gets about the body is head position and body length\n",
    "\n",
    "This environment might work eventually but I think it will take unreasonably long to train the model in this state \n",
    "\n",
    "The score is did not converge after after 45M steps, and averages around 6 points\n",
    "\n",
    "Making changes incrementally to rewards and observations is defiantly the way to go as you can quite clearly see if the changes is positive or negative\n",
    "\n",
    "Note to self: Model names should reflect the change made, distinguishing between models is hard when they are named PPO_v1_1653236185_960000: PPO_v1_linear_step_reward_1653236185_960000 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version I'm first going to make reward between food non-linear, even tho agent seemed to get to the food quite quickly in version two\n",
    "\n",
    "Non-linear reward can later be flipped after snake body has reached a larger size, this will in turn motivate agent to take longer paths to which might help the agent to not trap itself\n",
    "\n",
    "I'm also going to be adding a 5th action to see if snake takes more linear paths instead of diagonal paths, which again might help the agent when snake body gets longer\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "The non-linear reward seems to help the agent prioritize getting to the food more quickly, I have not yet implemented the inverse reward yet as the snake body is not getting much bigger than 30 segments\n",
    "\n",
    "After I added 5th action the model seemed to preform worse, which is understandable as it might increase the model complexity unnecessarily\n",
    "\n",
    "So I decided to reward the model for going straight lines. Either by rewarding the 5th action or by removing the 5th action and rewarding the agent for keeping it's previous direction\n",
    "\n",
    "Rewarding the agent for using the 5th action it seemed to backfire quite bad as the agent realized that if it just stays alive it can farm the reward for using the 5th action\n",
    "\n",
    "The models that only had 4 actions preformed better than the 5 action models again, amazingly none of those 3 models that I trained figured out the staying alive strategy\n",
    "\n",
    "The 4 action models did however seem to take more linear paths to the food, So for now I will keep the reward for keeping direction and remove the reward if I see it becoming a problem in the future versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model I want to give the agent more information about the snake body, I'm going to start with the easiest method of giving the agent a 1761 history of the head positions\n",
    "\n",
    "I will be doing this by increasing the observation space to 1764: history of steps using floats for x any y. For example x = 1, y = 3 is becomes a float of 1.3, hopefully this is enough for the agent to figure out that left of the decimal is x and right is y.\n",
    "\n",
    "My thinking is that this will reduce the observation space by half as if I would need another 1761 if I wanted to represent x and y individually, this is different than what people usually do"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d719b2e6fdfbb3f4419b871a02b73b42d117b79c667d3af1aa2d5e8c95fd0736"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('snake-reinforcement-learning-model-p9OIIo_b')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
