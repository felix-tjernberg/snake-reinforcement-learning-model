{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines 3\n",
    "\n",
    "## Swig installation\n",
    "\n",
    "Swig is \"only\" needed to run stable baselines 3 base environments and is not needed for custom game environments\n",
    "- [Install swig before installing baselines 3 on windows](https://gist.github.com/felix-tjernberg/8bc7313ad1a0de136789f11d7ae7acd3) \n",
    "- Install swig on mac before installing baselines 3 -> `brew install swig`\n",
    "\n",
    "## Tutorial references\n",
    "- [mustafa Video tutorial](https://www.youtube.com/watch?v=5dxJEXCjruE)\n",
    "- [sentdex Video tutorial](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)\n",
    "- [sentdex Written tutorial](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the goal was to learn the basic environment setup, so following cells are just code from the [first part of the sentex video tutorial](https://www.youtube.com/watch?v=XbWhJdQgi7E&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\") # Using CarPole-v1 as I could not get LunarLander to work on windows :/\n",
    "environment.reset()\n",
    "print(\n",
    "    f\"\"\"\n",
    "    sample action: {environment.action_space.sample()}\n",
    "    observation space: {environment.observation_space.shape}\n",
    "    sample observation: {environment.observation_space.sample()}\n",
    "    \"\"\"\n",
    ")\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "environment.reset()\n",
    "for step in range(100):\n",
    "    environment.render()\n",
    "    environment.step(environment.action_space.sample())\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.reset()\n",
    "for step in range(10):\n",
    "\tenvironment.render()\n",
    "\tobs, reward, done, info = environment.step(environment.action_space.sample())\n",
    "\tprint(obs, reward, done, info)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "A2C_model = A2C('MlpPolicy', environment, verbose=1)\n",
    "A2C_model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "\tobs = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\t# pass observation to model to get predicted action\n",
    "\t\taction, _states = A2C_model.predict(obs)\n",
    "\n",
    "\t\t# pass action to environment and get info back\n",
    "\t\tobs, rewards, done, info = environment.step(action)\n",
    "\n",
    "\t\t# show the environment on the screen\n",
    "\t\tenvironment.render()\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[In part 2 sentdex taught how to save and load models and run a local tensorboard website](https://www.youtube.com/watch?v=dLP-2Y6yu70&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "model_directories = ['models/A2C', 'models/PPO']\n",
    "log_directory = 'logs'\n",
    "\n",
    "for directory in model_directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "\n",
    "A2C_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory) # Switching to verbose 0 so we can train more without flooding the cell output\n",
    "PPO_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 10000\n",
    "def train_and_save_model(model, model_directory):\n",
    "    model_name = model_directory.split('/')[1]\n",
    "    for index in range(1,11):\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, reset_num_timesteps=False, tb_log_name=model_name)\n",
    "        model.save(f'{model_directory}/{TOTAL_TIMESTEPS*index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_model(A2C_model, model_directories[0])\n",
    "train_and_save_model(PPO_model, model_directories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching local tensorboard website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pipenv shell` then `tensorboard --logdir=logs` in terminal to serve a tensorboard website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and running a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model_path = f'{model_directories[0]}/90000.zip'\n",
    "\n",
    "loaded_PPO_model = PPO.load(ppo_model_path, environment)\n",
    "\n",
    "for episode in range(10):\n",
    "\tobservation = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\tenvironment.render()\n",
    "\t\taction, _ = loaded_PPO_model.predict(observation)\n",
    "\t\tobservation, reward, done, info = environment.step(action)\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making my snake game agent compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I [continued to watch part 3 from sentdex](https://www.youtube.com/watch?v=uKnjGn8fF70&list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1) and I also found [this video from mustafa](https://www.youtube.com/watch?v=5dxJEXCjruE) to see what was required when setting up the training environment\n",
    "\n",
    "What I noted from those videos was that they coded the snake game into the training environment class, which in my opinion makes the environment code very hard to read and edit\n",
    "\n",
    "My goal then became to rewrite the snake game into a class that is agent compatible, ie it can be ran from a training environment\n",
    "\n",
    "First I tried to add a variable that says if the game is agent driven and keep the human playability of the game which you can see in [this commit](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/commit/16e9a5915c538b5bcdf9743f758c351f47f97f6f). My thought process was that it might be cool if you could start the game either in agent mode or in human mode\n",
    "\n",
    "In the [next commit](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/commit/4526e9593bccd105e882c5b1e743be105b5880a0) I realized that it was just better to have a agent playable version and a human playable version of the game, to show that an agent could play I rewrote `if __name__ == \"__main__\":` so that it initializes the game and then tests if agent input works\n",
    "\n",
    "This accomplishes my goal of making both the snake game and training environment more readable and editable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal was to get the environment up and running in the most basic way like mustafa did and sentdex did in their videos, I combined mustafa's first observation with sentdex's observation code: `[head.x, head.y, food_delta_y, food_delta_x, body_x, body_y]`\n",
    "\n",
    "I also added body_x and body_y as observations. Which is the sum of all coordinates x and y which in my mind will create a vector for body size, shape and position: The thought is that hopefully model only need information about the body and not history of previous moves\n",
    "\n",
    "I also choose to do food_delta from sentdex's video instead of exact position from mustafa to help the model figure out if the food is up, down, left or right in relation to the head, which hopefully reduces the need for the model to figure that part out\n",
    "\n",
    "The check environment scripts that sentdex showed where very useful, especially the check_environment_visually (As I renamed his doublecheckenv script to). When I ran this script I saw that none of the inputs worked as I intended\n",
    "\n",
    "I solved the input issue by changing the Direction class into a dictionary as a class attribute instead in the snake game, so I learned the lesson of always checking the gym environment visually is very important which sentdex also said in his video :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and reward code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_game_state(self):\n",
    "    snake_head = self.snake_game.head\n",
    "    food = self.snake_game.food\n",
    "    food_delta_x = snake_head.x - food.x\n",
    "    food_delta_y = snake_head.y - food.y\n",
    "\n",
    "    all_body_x = [coordinate.x for coordinate in self.snake_game.body]\n",
    "    all_body_y = [coordinate.y for coordinate in self.snake_game.body]\n",
    "\n",
    "    return array(\n",
    "        [\n",
    "            snake_head.x,\n",
    "            snake_head.y,\n",
    "            food_delta_x,\n",
    "            food_delta_y,\n",
    "            sum(all_body_x),\n",
    "            sum(all_body_y),\n",
    "        ],\n",
    "        dtype=float32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward the agent for eating food and punish for game over\n",
    "if self.snake_game.game_over:\n",
    "    self.reward = self.snake_game.score - 10\n",
    "else:\n",
    "    self.reward = self.snake_game.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the first version was not very good it looked decent on [the graph](https://github.com/felix-tjernberg/snake-reinforcement-learning-model/blob/main/models/saved_models/PPO_v1_1653236185_960000.png), but when I ran the model it was just following some [spinning patterns](https://youtu.be/PGNiXGX2nLU?t=60) like mustafa warned it would as it really does not want to die\n",
    "\n",
    "But the goal was to get training and loading of models to work\n",
    "\n",
    "I'm saving the best model and logs for each environment change for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 of the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have set up the environment it is time to try different observations and rewards, I'm first going to try mustafas euclidean distance method and see what kind of results I can get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.2004514666935\n",
      "Coordinate(x=210.0, y=220.0) Coordinate(x=380, y=260)\n",
      "174.64249196572982\n",
      "Coordinate(x=200.0, y=220.0) Coordinate(x=380, y=260)\n",
      "184.39088914585776\n",
      "Coordinate(x=200.0, y=210.0) Coordinate(x=380, y=260)\n",
      "186.81541692269406\n",
      "Coordinate(x=210.0, y=210.0) Coordinate(x=380, y=260)\n",
      "177.2004514666935\n"
     ]
    }
   ],
   "source": [
    "from snake_game.snake_game_agent_version import Coordinate, SnekGame\n",
    "from math import sqrt\n",
    "from time import sleep\n",
    "\n",
    "def euclidean_distance(coordinate_one: Coordinate, coordinate_two: Coordinate):\n",
    "    \"\"\"Takes two coordinates and returns the euclidean distance between them\"\"\"\n",
    "    return sqrt((coordinate_one.x - coordinate_two.x) ** 2 + (coordinate_one.y - coordinate_two.y) ** 2)\n",
    "\n",
    "snake_game_instance = SnekGame()\n",
    "snake_game_instance.display_ui = True\n",
    "\n",
    "print(euclidean_distance(snake_game_instance.head, snake_game_instance.food))\n",
    "snake_game_instance.agent_action = snake_game_instance.DIRECTION[\"down\"]\n",
    "snake_game_instance.game_tick()\n",
    "sleep(1)\n",
    "print(snake_game_instance.head, snake_game_instance.food)\n",
    "print(euclidean_distance(snake_game_instance.head, snake_game_instance.food))\n",
    "snake_game_instance.agent_action = snake_game_instance.DIRECTION[\"left\"]\n",
    "snake_game_instance.game_tick()\n",
    "sleep(1)\n",
    "print(snake_game_instance.head, snake_game_instance.food)\n",
    "print(euclidean_distance(snake_game_instance.head, snake_game_instance.food))\n",
    "snake_game_instance.agent_action = snake_game_instance.DIRECTION[\"up\"]\n",
    "snake_game_instance.game_tick()\n",
    "sleep(1)\n",
    "print(snake_game_instance.head, snake_game_instance.food)\n",
    "print(euclidean_distance(snake_game_instance.head, snake_game_instance.food))\n",
    "snake_game_instance.agent_action = snake_game_instance.DIRECTION[\"right\"]\n",
    "snake_game_instance.game_tick()\n",
    "print(snake_game_instance.head, snake_game_instance.food)\n",
    "print(euclidean_distance(snake_game_instance.head, snake_game_instance.food))\n",
    "sleep(3)\n",
    "snake_game_instance.quit_game()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c098f4a38d42146aa7ff938a59f98e714f3b3ab9c1610561f6f3aa909ffef961"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('snake-reinforcement-learning-model-e-LfVFv8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
