{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines 3\n",
    "\n",
    "## Required installations to get their environments to work before installing baselines 3\n",
    "\n",
    "### Swig\n",
    "- [Install swig before installing baselines 3 on windows](https://gist.github.com/felix-tjernberg/8bc7313ad1a0de136789f11d7ae7acd3) \n",
    "- Install swig on mac before installing baselines 3 -> `brew install swig`\n",
    "\n",
    "## Tutorial reference\n",
    "- [Video tutorial](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1)\n",
    "- [Written tutorial](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Basic environment setup](https://www.youtube.com/watch?v=XbWhJdQgi7E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    sample action: 1\n",
      "    observation space: (4,)\n",
      "    sample observation: [3.9042451e+00 1.2297147e+38 3.3339074e-01 3.2759917e+36]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\") # Using CarPole-v1 as I could not get LunarLander to work on windows :/\n",
    "environment.reset()\n",
    "print(\n",
    "    f\"\"\"\n",
    "    sample action: {environment.action_space.sample()}\n",
    "    observation space: {environment.observation_space.shape}\n",
    "    sample observation: {environment.observation_space.sample()}\n",
    "    \"\"\"\n",
    ")\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CreativeZone2\\.virtualenvs\\snake-reinforcement-learning-model-e-LfVFv8\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"CartPole-v1\")\n",
    "environment.reset()\n",
    "for step in range(100):\n",
    "    environment.render()\n",
    "    environment.step(environment.action_space.sample())\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02662069  0.20854338  0.04225498 -0.23203783] 1.0 False {}\n",
      "[-0.02244982  0.40303686  0.03761422 -0.5110984 ] 1.0 False {}\n",
      "[-0.01438908  0.59760934  0.02739225 -0.7916947 ] 1.0 False {}\n",
      "[-0.0024369   0.7923447   0.01155836 -1.0756358 ] 1.0 False {}\n",
      "[ 0.01341     0.987312   -0.00995436 -1.3646692 ] 1.0 False {}\n",
      "[ 0.03315624  0.79231614 -0.03724774 -1.0751164 ] 1.0 False {}\n",
      "[ 0.04900256  0.98791    -0.05875007 -1.379252  ] 1.0 False {}\n",
      "[ 0.06876076  1.1837142  -0.08633511 -1.6897141 ] 1.0 False {}\n",
      "[ 0.09243505  1.3797213  -0.12012939 -2.0079808 ] 1.0 False {}\n",
      "[ 0.12002947  1.5758721  -0.160289   -2.3353195 ] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "for step in range(10):\n",
    "\tenvironment.render()\n",
    "\tobs, reward, done, info = environment.step(environment.action_space.sample())\n",
    "\tprint(obs, reward, done, info)\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CreativeZone2\\.virtualenvs\\snake-reinforcement-learning-model-e-LfVFv8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 19.1     |\n",
      "|    ep_rew_mean        | 19.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1018     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | -0.153   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.92     |\n",
      "|    value_loss         | 9.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 23.3     |\n",
      "|    ep_rew_mean        | 23.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1042     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | 0.0281   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 7.19     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x1799f0bf610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "A2C_model = A2C('MlpPolicy', environment, verbose=1)\n",
    "A2C_model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "\tobs = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\t# pass observation to model to get predicted action\n",
    "\t\taction, _states = A2C_model.predict(obs)\n",
    "\n",
    "\t\t# pass action to environment and get info back\n",
    "\t\tobs, rewards, done, info = environment.step(action)\n",
    "\n",
    "\t\t# show the environment on the screen\n",
    "\t\tenvironment.render()\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Model saving and loading](https://www.youtube.com/watch?v=dLP-2Y6yu70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "model_directories = ['models/A2C', 'models/PPO']\n",
    "log_directory = 'logs'\n",
    "\n",
    "for directory in model_directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "if not os.path.exists(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "\n",
    "A2C_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory) # Switching to verbose 0 so we can train more without flooding the cell output\n",
    "PPO_model = PPO('MlpPolicy', environment, verbose=0, tensorboard_log=log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 10000\n",
    "def train_and_save_model(model, model_directory):\n",
    "    model_name = model_directory.split('/')[1]\n",
    "    for index in range(1,11):\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, reset_num_timesteps=False, tb_log_name=model_name)\n",
    "        model.save(f'{model_directory}/{TOTAL_TIMESTEPS*index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_model(A2C_model, model_directories[0])\n",
    "train_and_save_model(PPO_model, model_directories[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pipenv shell` then `tensorboard --logdir=agent/logs` in terminal to serve a tensorboard website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model_path = f'{model_directories[0]}/90000.zip'\n",
    "\n",
    "loaded_PPO_model = PPO.load(ppo_model_path, environment)\n",
    "\n",
    "for episode in range(10):\n",
    "\tobservation = environment.reset()\n",
    "\tdone = False\n",
    "\twhile not done:\n",
    "\t\tenvironment.render()\n",
    "\t\taction, _ = loaded_PPO_model.predict(observation)\n",
    "\t\tobservation, reward, done, info = environment.step(action)\n",
    "environment.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c098f4a38d42146aa7ff938a59f98e714f3b3ab9c1610561f6f3aa909ffef961"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('snake-reinforcement-learning-model-e-LfVFv8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
